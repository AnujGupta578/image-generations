Attention mechanism 

Translate model

Generally used encoder and decoder

Attention mechanism -> Encoder-decoder 

Pass more data to decoder

take an extra step before creating output

1. What is the advantage of using the attention mechanism over a traditional recurrent neural network (RNN) encoder-decoder?

The attention mechanism lets the decoder focus on specific parts of the input sequence, which can improve the accuracy of the translation.

2. What is the purpose of the attention weights?

To assign weights to different parts of the input sequence, with the most important parts receiving the highest weights.

3.  What is the advantage of using the attention mechanism over a traditional sequence-to-sequence model?
The attention mechanism lets the model focus on specific parts of the input sequence.

4. How does an attention model differ from a traditional model?

Attention models pass a lot more information to the decoder.

5. What is the name of the machine learning technique that allows a neural network to focus on specific parts of an input sequence?


Attention mechanism

6. What is the name of the machine learning architecture that can be used to translate text from one language to another?

Encoder-decoder

